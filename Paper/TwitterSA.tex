\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{url}

% Include other packages here, before hyperref.
\usepackage{algorithm}
\usepackage{algpseudocode}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Comparison between a sequential and a distributed version of a Twitter sentiment analysis tool}

\author{
    Emilio Cecchini \\
    {\tt\small emilio.cecchini@stud.unifi.it}
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
In this paper I will analyze the advantages in speed and efficiency obtainable in implementing a Twitter sentiment analysis tool with Hadoop. The main goal of this study is not to build a perfect classifier, but it is instead to show the speedup that you can get using a distributed algorithm versus a sequential version. To build the sentiment classifier the library LingPipe is used, so the details of that classifier will be ignored. The distributed version is written in Hadoop with the MapReduce framework.
\end{abstract}

\section{Introduction}

Sentiment analysis involves classifying opinions in text into categories like \textit{positive} or \textit{negative} often with an implicit category of \textit{neutral}. In this project, for simplicity, we will ignore the \textit{neutral} category. All the entire part of building the classifier and getting results out of it is done with the LingPipe library, that is a Java library to perform machine learning tasks such as text classification and natural language processing.

First of all there is a \textit{training} phase, where the model is trained to recognize a tweet as negative or positive. This phase is only implemented in a sequential version. Once the model is built and trained, there is the \textit{classification} phase, where many unclassified tweets are passed as input to the model. This phase is implemented in two different versions: a sequential version and a distributed version in Hadoop with the MapReduce framework.

\section{Training}

The training process is done with a dataset containing tweets already classified. The model reads the tweet texts and their sentiment and try to learn how to perform the classification.

The process of training the model must be computed in a sequential way because every tweet and its associated sentiment must be processed by the same unique model. A parallel version could be hypothetical possible but in that case reading and processing the tweet must be encapsulated in a critical section and therefore all the advantages of parallelization would be lost.

Thanks to the LingPipe library, the algorithm of training the model is straightforward:

\begin{algorithm}[H]
\label{TrainingAlgorithm}
\caption{Training the model}
\begin{algorithmic}

    \State $m$ = initializeModel()
    \For{each tweet $t$}
        \State handle($m$, $t$)
    \EndFor
    \State saveModel($m$)

\end{algorithmic}
\end{algorithm}

The function \verb"saveModel()" save the model in a file so that it can be loaded later by a classifier to perform a classification.

\section{Classification}

The classification phase can be performed in a distributed way, so I have implemented a sequential version and a distributed version in Hadoop.

\subsection{Sequential version}

With the LingPipe library it is very easy to implement a sequential algorithm that classify tweets.

The first thing to do is to load the trained model. This process may take a long time if the model has been trained with a very large dataset. Loading the model can not be parallelized, so it is suggested to keep the file as small as possible to prevent it to become the bottleneck of the program.

After loading the model, the program iterates over the tweet texts and uses the classifier to predict their sentiment. During the iteration, the program keep tracks of the number of negative tweets as well as the number of positive tweets with two counters. The output of this algorithm are the two counters from which the general sentiment can be derived.

\begin{algorithm}[H]
\label{SequentialClassification}
\caption{Sequential classification}
\begin{algorithmic}

    \State $m$ = loadModel()
    \State $p$ = 0
    \State $n$ = 0
    \For{each tweet $t$}
        \State $s$ = classify($m$, $t$)
        \If{isPositiveSentiment($s$)}
            \State $p$ = $p$ + 1
        \Else
            \State $n$ = $n$ + 1
        \EndIf
    \EndFor
    \State \Return $p$, $n$

\end{algorithmic}
\end{algorithm}

\subsection{Distributed version}



\end{document}
